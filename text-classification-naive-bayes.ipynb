{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Text Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KxAEXrVJzH-B",
        "colab": {}
      },
      "source": [
        "# Please run this Jupyter file on Google Colab as the indentaion is according\n",
        "# to the google colab."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "657g91Q_YMr6",
        "colab": {}
      },
      "source": [
        "# This is used to import files from local system to Google Colab \n",
        "from google.colab import files\n",
        "import joblib"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o9fKbNtotKWO",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "70437c83-2c8b-4265-f13d-c0165fb6fe9d"
      },
      "source": [
        "# This is used to upload files to Google Colab\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7d684194-a500-4807-ba98-e341dabac298\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7d684194-a500-4807-ba98-e341dabac298\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving 20news-bydate.tar.gz to 20news-bydate.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XCNvujiW0A98",
        "colab": {}
      },
      "source": [
        "# This is used to extract the tar file\n",
        "!tar xvzf 20news-bydate.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ho1zK56p0DN-",
        "colab": {}
      },
      "source": [
        "# This is used to extract the tar file\n",
        "!tar xvzf mini_newsgroups.tar.gz"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c5EIxSEz4XuV",
        "colab": {}
      },
      "source": [
        "import string \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JVOSX6jwHQ12",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile,join\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "74JTgCVXNsnz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "1609e2d7-83a0-4ca8-af00-591089cc9a11"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nDv_c0410Gdj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1af74b36-acbd-4486-cfa7-0aa4b55fda1a"
      },
      "source": [
        "folder_path = '20news-bydate-train'\n",
        "\n",
        "files = []\n",
        "files.append([f for f in listdir(folder_path)])\n",
        "\n",
        "# checking total number of files gathered\n",
        "print(files)\n",
        "\n",
        "# texts contains all the documents present in each of the class\n",
        "texts=[]\n",
        "# documents contains the text written in each document\n",
        "documents = []\n",
        "# y_train contains the name of the class for each document\n",
        "y_train = []\n",
        "\n",
        "\n",
        "for file in files :\n",
        "    for file_name in file :\n",
        "        file_path = join(folder_path,file_name)\n",
        "        texts.append([f for f in listdir(file_path)])\n",
        "        doc = texts[len(texts)-1]\n",
        "        for f in doc :\n",
        "            # new_path gives us the path of each document\n",
        "            new_path = join(file_path,f)\n",
        "            d = open(new_path,encoding=\"utf8\", errors='ignore')\n",
        "            # lines contains the text in the document\n",
        "            lines = d.read()\n",
        "            # word_tokenize converts the sentences present in the document \n",
        "            # to the words\n",
        "            documents.append(word_tokenize(lines))\n",
        "            y_train.append(file_name)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['alt.atheism', 'sci.crypt', 'rec.autos', 'comp.sys.ibm.pc.hardware', 'comp.graphics', 'rec.sport.hockey', 'misc.forsale', 'talk.politics.mideast', 'talk.religion.misc', 'talk.politics.misc', 'soc.religion.christian', 'rec.sport.baseball', 'sci.med', 'comp.windows.x', 'sci.space', 'rec.motorcycles', 'comp.sys.mac.hardware', 'talk.politics.guns', 'comp.os.ms-windows.misc', 'sci.electronics']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-DT7J9wo9gav",
        "colab": {}
      },
      "source": [
        "# converting list to pandas\n",
        "y_train = pd.DataFrame({'col':y_train})"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OfIvEyOhHkpB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "9f526995-1a63-486a-d3b8-afe5d3f86d8d"
      },
      "source": [
        "# checking whether y_train is computed correctly or not\n",
        "print(y_train.iloc[1000,0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sci.crypt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YtmIkffaJYGd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "52da230f-693e-4381-aa4c-eb561489e3ce"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-vki9DTOH3WN",
        "colab": {}
      },
      "source": [
        "# storing stopwords in stop\n",
        "stop = stopwords.words('english')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pMxLZHyTzc44",
        "colab": {}
      },
      "source": [
        "# adding punctuations to stopwords\n",
        "punctuations = list(string.punctuation)\n",
        "stop = stop+punctuations"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kiLX1zHCJuxK",
        "colab": {}
      },
      "source": [
        "# Initialising lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PHX7mixUJxge",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "81556051-925b-46a2-ec1c-0aad7fa3886f"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o7wngVStJ0AB",
        "colab": {}
      },
      "source": [
        "# function to convert POS TAGS to simpler forms so that they can be\n",
        "# used in lemmatization\n",
        "\n",
        "def get_simple_pos(tag) :\n",
        "    # any pos_tag starting with J is adjective. It may be different \n",
        "    # types of adjectives\n",
        "    if tag.startswith('J') :\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V') :\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N') :\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R') :\n",
        "        return wordnet.ADV\n",
        "    # for simplicity we say that if none of them is there then it is a Noun\n",
        "    else :\n",
        "        return wordnet.NOUN"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mqd6uyb9J3Ae",
        "colab": {}
      },
      "source": [
        "# function to remove stop words and peform lemmatization in each document\n",
        "\n",
        "def clean_review(words) :\n",
        "    output_words=[]\n",
        "    for w in words :\n",
        "        if w.lower() not in stop :\n",
        "            # passing the word as it is important because if we pass the lowered\n",
        "            # case word then we loose some information about part of speech .\n",
        "            pos = pos_tag([w])\n",
        "            clean_word = lemmatizer.lemmatize(w,pos=get_simple_pos(pos[0][1]))\n",
        "            output_words.append(clean_word.lower())\n",
        "    return output_words"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DF2T2cBJKOpa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "09fc5a98-3f57-4459-e133-57890795193b"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0tBunVhcKSfZ",
        "colab": {}
      },
      "source": [
        "# calling clean_review function for each document in order to remove stop words\n",
        "# from each document and lemmatize the document\n",
        "documents = [clean_review(document) for document in documents]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "icHB3l-RKVYY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "bc7897c3-31d7-4e15-94bf-05b10f100b11"
      },
      "source": [
        "print(len(documents))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wqkXx3CwKYmr",
        "colab": {}
      },
      "source": [
        "# converting all the words present in each document to sentences\n",
        "text_documents = [\" \".join(document) for document in documents]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2C5IpjAgKaoc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b2c1574a-670e-4551-895c-ab621076c467"
      },
      "source": [
        "print(text_documents[0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bdunn cco.caltech.edu brendan dunn subject amusing atheist agnostic organization california institute technology pasadena lines 8 nntp-posting-host punisher.caltech.edu thanks whoever post wonderful parody people post without reading faq laugh good 5 minute part faq n't mention think might one two ... please n't tell n't joke 'm ready hear yet ... brendan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jISlq19pLY8M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "f3ced308-eb39-4f5d-8cc6-e5a501708aaa"
      },
      "source": [
        "# Using count vectoriser to create 2000 best features having ngram_range between\n",
        "# 1 and 2 i.e. we can have the features consiting of 1 word or 2 words\n",
        "\n",
        "count_vec = CountVectorizer(max_features = 2000,ngram_range=(1,2))\n",
        "# a here is a sparse matrix having 2000 columns as features\n",
        "a = count_vec.fit_transform(text_documents)\n",
        "# this is used to print this sparse matrix\n",
        "a.todense()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0, 0, 0, ..., 1, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 1, 0, 1],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0R2snjd-LfBB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "0f2413b0-f4e4-46b5-fcea-7335abf57bb4"
      },
      "source": [
        "a.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 2000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bf8bY1vmLiRI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f07c8b10-56e8-4af7-b8e1-a3fb81066bfa"
      },
      "source": [
        "# this is a list of 2000 features\n",
        "count_vec.get_feature_names()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['00',\n",
              " '000',\n",
              " '01',\n",
              " '02',\n",
              " '03',\n",
              " '04',\n",
              " '05',\n",
              " '06',\n",
              " '0d',\n",
              " '0d 0d',\n",
              " '0t',\n",
              " '0t 0t',\n",
              " '10',\n",
              " '100',\n",
              " '11',\n",
              " '12',\n",
              " '128',\n",
              " '13',\n",
              " '14',\n",
              " '145',\n",
              " '145 145',\n",
              " '15',\n",
              " '16',\n",
              " '17',\n",
              " '18',\n",
              " '19',\n",
              " '1988',\n",
              " '1990',\n",
              " '1991',\n",
              " '1992',\n",
              " '1993',\n",
              " '1993apr15',\n",
              " '1993apr16',\n",
              " '1993apr20',\n",
              " '1993apr5',\n",
              " '1d9',\n",
              " '1d9 1d9',\n",
              " '1st',\n",
              " '1t',\n",
              " '20',\n",
              " '200',\n",
              " '2000',\n",
              " '21',\n",
              " '22',\n",
              " '23',\n",
              " '24',\n",
              " '25',\n",
              " '250',\n",
              " '256',\n",
              " '26',\n",
              " '27',\n",
              " '28',\n",
              " '29',\n",
              " '2di',\n",
              " '2di 2di',\n",
              " '2nd',\n",
              " '2tm',\n",
              " '30',\n",
              " '300',\n",
              " '31',\n",
              " '32',\n",
              " '33',\n",
              " '34',\n",
              " '34u',\n",
              " '35',\n",
              " '36',\n",
              " '37',\n",
              " '38',\n",
              " '386',\n",
              " '39',\n",
              " '3d',\n",
              " '3t',\n",
              " '40',\n",
              " '400',\n",
              " '408',\n",
              " '41',\n",
              " '42',\n",
              " '43',\n",
              " '44',\n",
              " '45',\n",
              " '46',\n",
              " '47',\n",
              " '48',\n",
              " '486',\n",
              " '49',\n",
              " '50',\n",
              " '500',\n",
              " '51',\n",
              " '52',\n",
              " '53',\n",
              " '54',\n",
              " '55',\n",
              " '56',\n",
              " '57',\n",
              " '58',\n",
              " '59',\n",
              " '60',\n",
              " '61',\n",
              " '64',\n",
              " '65',\n",
              " '66',\n",
              " '6ei',\n",
              " '6um',\n",
              " '70',\n",
              " '71',\n",
              " '72',\n",
              " '75',\n",
              " '75u',\n",
              " '75u 75u',\n",
              " '76',\n",
              " '7ey',\n",
              " '7u',\n",
              " '80',\n",
              " '800',\n",
              " '81',\n",
              " '82',\n",
              " '85',\n",
              " '86',\n",
              " '89',\n",
              " '90',\n",
              " '91',\n",
              " '92',\n",
              " '93',\n",
              " '95',\n",
              " '9v',\n",
              " '__',\n",
              " '__ __',\n",
              " '___',\n",
              " '___ ___',\n",
              " '_____',\n",
              " '_o',\n",
              " 'a86',\n",
              " 'a86 a86',\n",
              " 'ab',\n",
              " 'ability',\n",
              " 'able',\n",
              " 'absolute',\n",
              " 'absolutely',\n",
              " 'abuse',\n",
              " 'ac',\n",
              " 'ac uk',\n",
              " 'accept',\n",
              " 'accepted',\n",
              " 'access',\n",
              " 'access digex',\n",
              " 'accord',\n",
              " 'account',\n",
              " 'across',\n",
              " 'acs',\n",
              " 'acs ohio',\n",
              " 'act',\n",
              " 'action',\n",
              " 'activity',\n",
              " 'actual',\n",
              " 'actually',\n",
              " 'ad',\n",
              " 'adam',\n",
              " 'add',\n",
              " 'addition',\n",
              " 'additional',\n",
              " 'address',\n",
              " 'administration',\n",
              " 'admit',\n",
              " 'advance',\n",
              " 'advantage',\n",
              " 'advice',\n",
              " 'age',\n",
              " 'agency',\n",
              " 'agent',\n",
              " 'ago',\n",
              " 'agree',\n",
              " 'ah',\n",
              " 'ai',\n",
              " 'air',\n",
              " 'al',\n",
              " 'alan',\n",
              " 'alaska',\n",
              " 'algorithm',\n",
              " 'alive',\n",
              " 'all',\n",
              " 'allen',\n",
              " 'allow',\n",
              " 'allows',\n",
              " 'almost',\n",
              " 'alone',\n",
              " 'along',\n",
              " 'already',\n",
              " 'also',\n",
              " 'alt',\n",
              " 'alternative',\n",
              " 'although',\n",
              " 'always',\n",
              " 'amendment',\n",
              " 'america',\n",
              " 'american',\n",
              " 'americans',\n",
              " 'among',\n",
              " 'amount',\n",
              " 'analysis',\n",
              " 'and',\n",
              " 'and or',\n",
              " 'andrew',\n",
              " 'andrew cmu',\n",
              " 'andy',\n",
              " 'angeles',\n",
              " 'animal',\n",
              " 'announce',\n",
              " 'announcement',\n",
              " 'anonymous',\n",
              " 'anonymous ftp',\n",
              " 'another',\n",
              " 'answer',\n",
              " 'answer question',\n",
              " 'anti',\n",
              " 'anybody',\n",
              " 'anyone',\n",
              " 'anyone else',\n",
              " 'anyone know',\n",
              " 'anything',\n",
              " 'anyway',\n",
              " 'apparently',\n",
              " 'appear',\n",
              " 'apple',\n",
              " 'apple com',\n",
              " 'application',\n",
              " 'apply',\n",
              " 'appreciate',\n",
              " 'approach',\n",
              " 'appropriate',\n",
              " 'apr',\n",
              " 'apr 1993',\n",
              " 'apr 93',\n",
              " 'april',\n",
              " 'arab',\n",
              " 'archive',\n",
              " 'area',\n",
              " 'argic',\n",
              " 'argue',\n",
              " 'argument',\n",
              " 'arizona',\n",
              " 'arm',\n",
              " 'armenia',\n",
              " 'armenian',\n",
              " 'armenians',\n",
              " 'army',\n",
              " 'around',\n",
              " 'art',\n",
              " 'article',\n",
              " 'article 1993apr15',\n",
              " 'article 1993apr20',\n",
              " 'article apr',\n",
              " 'as',\n",
              " 'ask',\n",
              " 'associate',\n",
              " 'assume',\n",
              " 'atheism',\n",
              " 'atheist',\n",
              " 'athena',\n",
              " 'att',\n",
              " 'att com',\n",
              " 'attack',\n",
              " 'attempt',\n",
              " 'au',\n",
              " 'audio',\n",
              " 'austin',\n",
              " 'australia',\n",
              " 'author',\n",
              " 'authority',\n",
              " 'auto',\n",
              " 'automatic',\n",
              " 'available',\n",
              " 'average',\n",
              " 'avoid',\n",
              " 'aware',\n",
              " 'away',\n",
              " 'ax',\n",
              " 'ax ax',\n",
              " 'ax max',\n",
              " 'b8f',\n",
              " 'b8f b8f',\n",
              " 'back',\n",
              " 'background',\n",
              " 'bad',\n",
              " 'ball',\n",
              " 'ban',\n",
              " 'bank',\n",
              " 'banks',\n",
              " 'bar',\n",
              " 'base',\n",
              " 'baseball',\n",
              " 'based',\n",
              " 'basic',\n",
              " 'basically',\n",
              " 'basis',\n",
              " 'batf',\n",
              " 'battery',\n",
              " 'bbs',\n",
              " 'bc',\n",
              " 'bear',\n",
              " 'beat',\n",
              " 'become',\n",
              " 'begin',\n",
              " 'behavior',\n",
              " 'behind',\n",
              " 'belief',\n",
              " 'believe',\n",
              " 'bell',\n",
              " 'benefit',\n",
              " 'berkeley',\n",
              " 'berkeley edu',\n",
              " 'besides',\n",
              " 'best',\n",
              " 'beyond',\n",
              " 'bh',\n",
              " 'bhj',\n",
              " 'bible',\n",
              " 'big',\n",
              " 'bike',\n",
              " 'bill',\n",
              " 'billion',\n",
              " 'bios',\n",
              " 'bit',\n",
              " 'bitnet',\n",
              " 'bj',\n",
              " 'black',\n",
              " 'blame',\n",
              " 'block',\n",
              " 'blood',\n",
              " 'blue',\n",
              " 'bmw',\n",
              " 'bnr',\n",
              " 'bnr ca',\n",
              " 'board',\n",
              " 'bob',\n",
              " 'body',\n",
              " 'book',\n",
              " 'boot',\n",
              " 'border',\n",
              " 'born',\n",
              " 'bos',\n",
              " 'boston',\n",
              " 'bother',\n",
              " 'bought',\n",
              " 'boulder',\n",
              " 'box',\n",
              " 'boy',\n",
              " 'brad',\n",
              " 'brain',\n",
              " 'brand',\n",
              " 'break',\n",
              " 'brian',\n",
              " 'bring',\n",
              " 'british',\n",
              " 'brother',\n",
              " 'brought',\n",
              " 'brown',\n",
              " 'bruce',\n",
              " 'btw',\n",
              " 'bu',\n",
              " 'budget',\n",
              " 'buf',\n",
              " 'buffalo',\n",
              " 'bug',\n",
              " 'build',\n",
              " 'building',\n",
              " 'built',\n",
              " 'bunch',\n",
              " 'burn',\n",
              " 'bus',\n",
              " 'business',\n",
              " 'button',\n",
              " 'buy',\n",
              " 'buying',\n",
              " 'bxn',\n",
              " 'byte',\n",
              " 'c_',\n",
              " 'ca',\n",
              " 'ca lines',\n",
              " 'cable',\n",
              " 'cache',\n",
              " 'cal',\n",
              " 'california',\n",
              " 'call',\n",
              " 'caltech',\n",
              " 'caltech edu',\n",
              " 'cambridge',\n",
              " 'canada',\n",
              " 'capability',\n",
              " 'car',\n",
              " 'card',\n",
              " 'care',\n",
              " 'carry',\n",
              " 'case',\n",
              " 'catholic',\n",
              " 'cause',\n",
              " 'cb',\n",
              " 'cb att',\n",
              " 'cc',\n",
              " 'cc columbia',\n",
              " 'cco',\n",
              " 'cd',\n",
              " 'center',\n",
              " 'central',\n",
              " 'century',\n",
              " 'certain',\n",
              " 'certainly',\n",
              " 'ch',\n",
              " 'chance',\n",
              " 'change',\n",
              " 'channel',\n",
              " 'character',\n",
              " 'charge',\n",
              " 'charles',\n",
              " 'cheap',\n",
              " 'check',\n",
              " 'chicago',\n",
              " 'child',\n",
              " 'chip',\n",
              " 'choice',\n",
              " 'choose',\n",
              " 'chris',\n",
              " 'christ',\n",
              " 'christian',\n",
              " 'christianity',\n",
              " 'christians',\n",
              " 'church',\n",
              " 'chz',\n",
              " 'circuit',\n",
              " 'cis',\n",
              " 'citizen',\n",
              " 'city',\n",
              " 'civil',\n",
              " 'civilian',\n",
              " 'claim',\n",
              " 'class',\n",
              " 'clear',\n",
              " 'clearly',\n",
              " 'cleveland',\n",
              " 'cleveland freenet',\n",
              " 'client',\n",
              " 'clinton',\n",
              " 'clipper',\n",
              " 'clipper chip',\n",
              " 'clock',\n",
              " 'close',\n",
              " 'club',\n",
              " 'cmu',\n",
              " 'cmu edu',\n",
              " 'co',\n",
              " 'co uk',\n",
              " 'code',\n",
              " 'cold',\n",
              " 'college',\n",
              " 'color',\n",
              " 'colorado',\n",
              " 'colorado edu',\n",
              " 'columbia',\n",
              " 'columbia edu',\n",
              " 'com',\n",
              " 'com article',\n",
              " 'com jim',\n",
              " 'com organization',\n",
              " 'com steve',\n",
              " 'com subject',\n",
              " 'com writes',\n",
              " 'come',\n",
              " 'command',\n",
              " 'comment',\n",
              " 'commercial',\n",
              " 'commit',\n",
              " 'committee',\n",
              " 'common',\n",
              " 'communication',\n",
              " 'communications',\n",
              " 'community',\n",
              " 'comp',\n",
              " 'company',\n",
              " 'compare',\n",
              " 'compatible',\n",
              " 'compile',\n",
              " 'complete',\n",
              " 'completely',\n",
              " 'compound',\n",
              " 'computer',\n",
              " 'computer science',\n",
              " 'computing',\n",
              " 'concept',\n",
              " 'concern',\n",
              " 'concerned',\n",
              " 'conclusion',\n",
              " 'condition',\n",
              " 'conference',\n",
              " 'conflict',\n",
              " 'confuse',\n",
              " 'congress',\n",
              " 'connect',\n",
              " 'connection',\n",
              " 'consider',\n",
              " 'constitution',\n",
              " 'contact',\n",
              " 'contain',\n",
              " 'contains',\n",
              " 'content',\n",
              " 'context',\n",
              " 'continue',\n",
              " 'control',\n",
              " 'controller',\n",
              " 'convert',\n",
              " 'convince',\n",
              " 'cool',\n",
              " 'cop',\n",
              " 'copy',\n",
              " 'cornell',\n",
              " 'corp',\n",
              " 'corporation',\n",
              " 'correct',\n",
              " 'cost',\n",
              " 'could',\n",
              " 'count',\n",
              " 'country',\n",
              " 'couple',\n",
              " 'course',\n",
              " 'court',\n",
              " 'cover',\n",
              " 'cpu',\n",
              " 'craig',\n",
              " 'cramer',\n",
              " 'create',\n",
              " 'crime',\n",
              " 'criminal',\n",
              " 'cross',\n",
              " 'crypto',\n",
              " 'cryptography',\n",
              " 'cs',\n",
              " 'csd',\n",
              " 'cso',\n",
              " 'cso uiuc',\n",
              " 'cup',\n",
              " 'current',\n",
              " 'currently',\n",
              " 'cut',\n",
              " 'cwru',\n",
              " 'cwru edu',\n",
              " 'cx',\n",
              " 'd9',\n",
              " 'dale',\n",
              " 'damage',\n",
              " 'damn',\n",
              " 'dan',\n",
              " 'dangerous',\n",
              " 'daniel',\n",
              " 'data',\n",
              " 'date',\n",
              " 'dave',\n",
              " 'david',\n",
              " 'day',\n",
              " 'db',\n",
              " 'dc',\n",
              " 'de',\n",
              " 'dead',\n",
              " 'deal',\n",
              " 'dealer',\n",
              " 'death',\n",
              " 'dec',\n",
              " 'decide',\n",
              " 'decision',\n",
              " 'default',\n",
              " 'defend',\n",
              " 'defense',\n",
              " 'define',\n",
              " 'definition',\n",
              " 'degree',\n",
              " 'delete',\n",
              " 'demand',\n",
              " 'deny',\n",
              " 'department',\n",
              " 'depend',\n",
              " 'dept',\n",
              " 'des',\n",
              " 'described',\n",
              " 'description',\n",
              " 'design',\n",
              " 'desire',\n",
              " 'detail',\n",
              " 'determine',\n",
              " 'detroit',\n",
              " 'develop',\n",
              " 'developed',\n",
              " 'development',\n",
              " 'device',\n",
              " 'die',\n",
              " 'difference',\n",
              " 'different',\n",
              " 'difficult',\n",
              " 'digex',\n",
              " 'digital',\n",
              " 'direct',\n",
              " 'direction',\n",
              " 'directly',\n",
              " 'directory',\n",
              " 'disclaimer',\n",
              " 'discuss',\n",
              " 'discussion',\n",
              " 'disease',\n",
              " 'disk',\n",
              " 'display',\n",
              " 'distribution',\n",
              " 'distribution na',\n",
              " 'distribution usa',\n",
              " 'distribution world',\n",
              " 'division',\n",
              " 'do',\n",
              " 'doctor',\n",
              " 'document',\n",
              " 'dod',\n",
              " 'dog',\n",
              " 'dollar',\n",
              " 'domain',\n",
              " 'door',\n",
              " 'dos',\n",
              " 'double',\n",
              " 'doubt',\n",
              " 'doug',\n",
              " 'douglas',\n",
              " 'dr',\n",
              " 'draft',\n",
              " 'draw',\n",
              " 'drive',\n",
              " 'driver',\n",
              " 'drop',\n",
              " 'drug',\n",
              " 'du',\n",
              " 'due',\n",
              " 'duke',\n",
              " 'earlier',\n",
              " 'early',\n",
              " 'earth',\n",
              " 'easily',\n",
              " 'east',\n",
              " 'easy',\n",
              " 'eat',\n",
              " 'ecn',\n",
              " 'economic',\n",
              " 'ed',\n",
              " 'edge',\n",
              " 'editor',\n",
              " 'edu',\n",
              " 'edu article',\n",
              " 'edu au',\n",
              " 'edu david',\n",
              " 'edu john',\n",
              " 'edu keith',\n",
              " 'edu michael',\n",
              " 'edu organization',\n",
              " 'edu subject',\n",
              " 'edu writes',\n",
              " 'education',\n",
              " 'ee',\n",
              " 'eff',\n",
              " 'effect',\n",
              " 'effective',\n",
              " 'effort',\n",
              " 'either',\n",
              " 'electrical',\n",
              " 'electronic',\n",
              " 'electronics',\n",
              " 'else',\n",
              " 'email',\n",
              " 'encrypt',\n",
              " 'encryption',\n",
              " 'end',\n",
              " 'energy',\n",
              " 'enforcement',\n",
              " 'eng',\n",
              " 'engine',\n",
              " 'engineer',\n",
              " 'engineering',\n",
              " 'english',\n",
              " 'enough',\n",
              " 'enter',\n",
              " 'entire',\n",
              " 'entirely',\n",
              " 'entry',\n",
              " 'environment',\n",
              " 'equipment',\n",
              " 'eric',\n",
              " 'error',\n",
              " 'escape',\n",
              " 'escrow',\n",
              " 'especially',\n",
              " 'establish',\n",
              " 'et',\n",
              " 'etc',\n",
              " 'eternal',\n",
              " 'europe',\n",
              " 'european',\n",
              " 'even',\n",
              " 'even though',\n",
              " 'event',\n",
              " 'ever',\n",
              " 'every',\n",
              " 'everybody',\n",
              " 'everyone',\n",
              " 'everything',\n",
              " 'evidence',\n",
              " 'evil',\n",
              " 'exact',\n",
              " 'exactly',\n",
              " 'example',\n",
              " 'excellent',\n",
              " 'except',\n",
              " 'excuse',\n",
              " 'exist',\n",
              " 'existence',\n",
              " 'exists',\n",
              " 'expansion',\n",
              " 'expect',\n",
              " 'expensive',\n",
              " 'experience',\n",
              " 'expert',\n",
              " 'explain',\n",
              " 'export',\n",
              " 'express',\n",
              " 'external',\n",
              " 'extra',\n",
              " 'eye',\n",
              " 'face',\n",
              " 'facility',\n",
              " 'fact',\n",
              " 'factor',\n",
              " 'fail',\n",
              " 'fair',\n",
              " 'fairly',\n",
              " 'faith',\n",
              " 'fall',\n",
              " 'false',\n",
              " 'family',\n",
              " 'fan',\n",
              " 'faq',\n",
              " 'far',\n",
              " 'fast',\n",
              " 'faster',\n",
              " 'father',\n",
              " 'fault',\n",
              " 'fax',\n",
              " 'fbi',\n",
              " 'fear',\n",
              " 'feature',\n",
              " 'federal',\n",
              " 'feel',\n",
              " 'fi',\n",
              " 'field',\n",
              " 'fight',\n",
              " 'figure',\n",
              " 'file',\n",
              " 'fill',\n",
              " 'final',\n",
              " 'finally',\n",
              " 'find',\n",
              " 'fine',\n",
              " 'finish',\n",
              " 'fire',\n",
              " 'firearm',\n",
              " 'first',\n",
              " 'fit',\n",
              " 'five',\n",
              " 'fix',\n",
              " 'flame',\n",
              " 'flight',\n",
              " 'floppy',\n",
              " 'folk',\n",
              " 'follow',\n",
              " 'font',\n",
              " 'food',\n",
              " 'foot',\n",
              " 'force',\n",
              " 'ford',\n",
              " 'forget',\n",
              " 'form',\n",
              " 'format',\n",
              " 'former',\n",
              " 'forward',\n",
              " 'found',\n",
              " 'foundation',\n",
              " 'four',\n",
              " 'frame',\n",
              " 'frank',\n",
              " 'fred',\n",
              " 'free',\n",
              " 'freedom',\n",
              " 'freenet',\n",
              " 'freenet edu',\n",
              " 'frequently',\n",
              " 'friend',\n",
              " 'front',\n",
              " 'ftp',\n",
              " 'full',\n",
              " 'fully',\n",
              " 'fun',\n",
              " 'function',\n",
              " 'fund',\n",
              " 'future',\n",
              " 'g9v',\n",
              " 'g9v g9v',\n",
              " 'gain',\n",
              " 'game',\n",
              " 'gary',\n",
              " 'gas',\n",
              " 'gatech',\n",
              " 'gatech edu',\n",
              " 'gateway',\n",
              " 'gay',\n",
              " 'geb',\n",
              " 'general',\n",
              " 'generally',\n",
              " 'generate',\n",
              " 'genocide',\n",
              " 'george',\n",
              " 'georgia',\n",
              " 'german',\n",
              " 'germany',\n",
              " 'get',\n",
              " 'gif',\n",
              " 'give',\n",
              " 'giz',\n",
              " 'gk',\n",
              " 'gm',\n",
              " 'gmt',\n",
              " 'go',\n",
              " 'goal',\n",
              " 'god',\n",
              " 'good',\n",
              " 'gordon',\n",
              " 'gordon banks',\n",
              " 'gov',\n",
              " 'government',\n",
              " 'grant',\n",
              " 'graphic',\n",
              " 'graphics',\n",
              " 'great',\n",
              " 'greatly',\n",
              " 'greek',\n",
              " 'green',\n",
              " 'greg',\n",
              " 'ground',\n",
              " 'group',\n",
              " 'guess',\n",
              " 'guest',\n",
              " 'guide',\n",
              " 'gun',\n",
              " 'gun control',\n",
              " 'guy',\n",
              " 'half',\n",
              " 'hall',\n",
              " 'hand',\n",
              " 'handgun',\n",
              " 'handle',\n",
              " 'happen',\n",
              " 'happens',\n",
              " 'happy',\n",
              " 'hard',\n",
              " 'hard disk',\n",
              " 'hard drive',\n",
              " 'hardware',\n",
              " 'harvard',\n",
              " 'harvard edu',\n",
              " 'hate',\n",
              " 'hd',\n",
              " 'head',\n",
              " 'health',\n",
              " 'hear',\n",
              " 'heard',\n",
              " 'heart',\n",
              " 'heaven',\n",
              " 'heavy',\n",
              " 'held',\n",
              " 'hell',\n",
              " 'hello',\n",
              " 'help',\n",
              " 'henry',\n",
              " 'hey',\n",
              " 'hi',\n",
              " 'high',\n",
              " 'highly',\n",
              " 'history',\n",
              " 'hit',\n",
              " 'hockey',\n",
              " 'hold',\n",
              " 'hole',\n",
              " 'holy',\n",
              " 'home',\n",
              " 'homosexual',\n",
              " 'hope',\n",
              " 'hospital',\n",
              " 'host',\n",
              " 'hot',\n",
              " 'hour',\n",
              " 'house',\n",
              " 'however',\n",
              " 'hp',\n",
              " 'hp com',\n",
              " 'human',\n",
              " 'hundred',\n",
              " 'hurt',\n",
              " 'hz',\n",
              " 'iastate',\n",
              " 'iastate edu',\n",
              " 'ibm',\n",
              " 'ibm com',\n",
              " 'ice',\n",
              " 'id',\n",
              " 'ide',\n",
              " 'idea',\n",
              " 'ie',\n",
              " 'ignore',\n",
              " 'ii',\n",
              " 'il',\n",
              " 'illegal',\n",
              " 'illinois',\n",
              " 'image',\n",
              " 'imagine',\n",
              " 'implement',\n",
              " 'important',\n",
              " 'impossible',\n",
              " 'improve',\n",
              " 'in',\n",
              " 'in reply',\n",
              " 'inc',\n",
              " 'inc lines',\n",
              " 'include',\n",
              " 'increase',\n",
              " 'indeed',\n",
              " 'independent',\n",
              " 'indiana',\n",
              " 'indiana edu',\n",
              " 'individual',\n",
              " 'industry',\n",
              " 'info',\n",
              " 'informatik',\n",
              " 'information',\n",
              " 'injury',\n",
              " 'innocent',\n",
              " 'input',\n",
              " 'ins',\n",
              " 'ins cwru',\n",
              " 'inside',\n",
              " 'instal',\n",
              " 'instance',\n",
              " 'instead',\n",
              " 'institute',\n",
              " 'institute technology',\n",
              " 'instruction',\n",
              " 'insurance',\n",
              " 'int',\n",
              " 'intend',\n",
              " 'interest',\n",
              " 'interested',\n",
              " 'interface',\n",
              " 'internal',\n",
              " 'international',\n",
              " 'internet',\n",
              " 'interpretation',\n",
              " 'involve',\n",
              " 'is',\n",
              " 'isa',\n",
              " 'isc',\n",
              " 'islam',\n",
              " 'islamic',\n",
              " 'israel',\n",
              " 'israeli',\n",
              " 'issue',\n",
              " 'it',\n",
              " 'item',\n",
              " 'jack',\n",
              " 'james',\n",
              " 'jason',\n",
              " 'jeff',\n",
              " 'jesus',\n",
              " 'jewish',\n",
              " 'jews',\n",
              " 'jim',\n",
              " 'job',\n",
              " 'joe',\n",
              " 'john',\n",
              " 'johnson',\n",
              " 'join',\n",
              " 'jon',\n",
              " 'jose',\n",
              " 'joseph',\n",
              " 'jpeg',\n",
              " 'jpl',\n",
              " 'jpl nasa',\n",
              " 'jr',\n",
              " 'judge',\n",
              " 'justice',\n",
              " 'keep',\n",
              " 'keith',\n",
              " 'ken',\n",
              " 'kent',\n",
              " 'kept',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CycuyZ-qKBqC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "40ca3e30-60c3-4fd2-d622-bc775435b29e"
      },
      "source": [
        "folder_path = '20news-bydate-test'\n",
        "\n",
        "files = []\n",
        "files.append([f for f in listdir(folder_path)])\n",
        "# checking total number of files gathered\n",
        "print(files)\n",
        "\n",
        "# texts contains all the documents present in each of the class\n",
        "texts=[]\n",
        "# test_documents contains the text written in each document\n",
        "test_documents = []\n",
        "# y_test contains the name of the class for each document\n",
        "y_test = []\n",
        "\n",
        "\n",
        "for file in files :\n",
        "    for file_name in file :\n",
        "        file_path = join(folder_path,file_name)\n",
        "        texts.append([f for f in listdir(file_path)])\n",
        "        doc = texts[len(texts)-1]\n",
        "        for f in doc :\n",
        "            # new_path gives us the path of each document\n",
        "            new_path = join(file_path,f)\n",
        "            d = open(new_path,encoding=\"utf8\", errors='ignore')\n",
        "            # lines contains the text in the document\n",
        "            lines = d.read()\n",
        "            # word_tokenize converts the sentences present in the test_documents \n",
        "            # to the words\n",
        "            test_documents.append(word_tokenize(lines))\n",
        "            y_test.append(file_name)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['alt.atheism', 'sci.crypt', 'rec.autos', 'comp.sys.ibm.pc.hardware', 'comp.graphics', 'rec.sport.hockey', 'misc.forsale', 'talk.politics.mideast', 'talk.religion.misc', 'talk.politics.misc', 'soc.religion.christian', 'rec.sport.baseball', 'sci.med', 'comp.windows.x', 'sci.space', 'rec.motorcycles', 'comp.sys.mac.hardware', 'talk.politics.guns', 'comp.os.ms-windows.misc', 'sci.electronics']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wzI0DdfTQmDd",
        "colab": {}
      },
      "source": [
        "# converting list to pandas\n",
        "y_test = pd.DataFrame({'col':y_test})"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oToYdA51QsjK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "cb3543fc-8603-4ff7-ad4c-f71941de583c"
      },
      "source": [
        "# checking whether y_test is correctly computed or not\n",
        "print(y_test.iloc[1000,0])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rec.autos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zTyVEJ_7Mpqc",
        "colab": {}
      },
      "source": [
        "# calling clean_review function for each document in order to remove stop words\n",
        "# from each document and lemmatize the document\n",
        "test_documents = [clean_review(document) for document in test_documents]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a2W0GolpN0zU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "398875cc-3ae5-44f1-f609-b1e8188bc60f"
      },
      "source": [
        "print(len(test_documents))\n",
        "print(y_test.shape)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7532\n",
            "(7532, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GGfR-o34MduY",
        "colab": {}
      },
      "source": [
        "# converting all the words present in each document to sentences\n",
        "test_text_documents = [\" \".join(document) for document in test_documents]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B-sOSUkFOAI6",
        "colab": {}
      },
      "source": [
        "# count vectoriser here gives us the most common \n",
        "# 2000 features for the test documents\n",
        "x_test_features = count_vec.transform(test_text_documents)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qh-h3R5BObhW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "bc1bf8c4-3002-41ae-a417-1a6fdea8bf4d"
      },
      "source": [
        "x_test_features.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7532, 2000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gWx69du-Td11",
        "colab": {}
      },
      "source": [
        "# fit function is used to create and return a dictionary result that can be \n",
        "# used to find the number of occurences of a word in a particular document .\n",
        "\n",
        "def fit(X_train,Y_train) :\n",
        "    # result is a dictionary\n",
        "    result = {}\n",
        "    # class_values are the different values that Y_train can take\n",
        "    class_values = set(Y_train.iloc[:,0])\n",
        "  \n",
        "    # sum stores the sum of occurrences of all words in all documents\n",
        "    sum = 0\n",
        "    for i in range(X_train.shape[1]) :\n",
        "      sum = sum + X_train.iloc[:,i].sum()\n",
        "    result[\"total_data\"] = sum\n",
        "    print(class_values)\n",
        "    \n",
        "    \n",
        "    for current_class in class_values :\n",
        "      result[current_class] = {}\n",
        "      # current_class_rows contains those rows from X_train that \n",
        "      # have current_class as output in Y_train\n",
        "      current_class_rows = (Y_train==current_class)\n",
        "      j=0\n",
        "      X_train_current = pd.DataFrame(columns=X_train.columns)\n",
        "      for i in range(len(current_class_rows)) :\n",
        "        if current_class_rows.iloc[i,0]==True :\n",
        "          X_train_current.loc[j] = X_train.iloc[i,:]\n",
        "          j=j+1\n",
        "      j=0\n",
        "      Y_train_current = pd.DataFrame(columns=Y_train.columns)\n",
        "      for i in range(len(current_class_rows)) :\n",
        "        if current_class_rows.iloc[i,0]==True :\n",
        "          Y_train_current.loc[j] = Y_train.iloc[i,:]\n",
        "          j=j+1\n",
        "    \n",
        "   \n",
        "      # total is used to store the occurrences of all the words in the documents\n",
        "      # belonging to a particular class\n",
        "      total = 0\n",
        "      for i in range(X_train_current.shape[1]) :\n",
        "        total = total + X_train_current.iloc[:,i].sum()\n",
        "      result[current_class][\"total_count\"] = total\n",
        "    \n",
        "      # num_features stores the total number of features  \n",
        "      num_features = X_train.shape[1]\n",
        "      for j in range(0,num_features) :\n",
        "        feature_sum = X_train_current.iloc[:,j].sum()\n",
        "        result[current_class][j] = feature_sum\n",
        "      # return the dictionary result\n",
        "      \n",
        "    return result   "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZY86KQLnG9PF",
        "colab": {}
      },
      "source": [
        "# this function returns the probability of the given document x belonging\n",
        "# to the current_class\n",
        "\n",
        "def probability(dictionary, x, current_class):\n",
        "    features = count_vec.get_feature_names()\n",
        "    # here i have used logarithmic probability\n",
        "    output = np.log(dictionary[current_class][\"total_count\"]) - np.log(dictionary[\"total_data\"])\n",
        "    for word in x :\n",
        "      # find the probability of a word only if it is present in features\n",
        "      if word in features :\n",
        "        for i in range(len(features)) :\n",
        "          if(features[i]==word) :\n",
        "            break\n",
        "        # 1 is added for laplace correction\n",
        "        num = dictionary[current_class][i]+1\n",
        "        # len(dictionary[current_class].keys()) is added for laplace correction\n",
        "        den = dictionary[current_class][\"total_count\"] + len(dictionary[current_class].keys())\n",
        "        current_probablity = np.log(num) - np.log(den)\n",
        "        output = output + current_probablity\n",
        "    return output"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uucz6OOzHDA2",
        "colab": {}
      },
      "source": [
        "# this function is used to find and return the best class for each point in the\n",
        "# test document\n",
        "def predictSinglePoint(dictionary, x):\n",
        "    # this gives us all the keys of this dictionary\n",
        "    classes = dictionary.keys()\n",
        "    best_p = -1000\n",
        "    best_class = -1\n",
        "    first_run = True\n",
        "    for current_class in classes:\n",
        "        if (current_class == \"total_data\"):\n",
        "            continue\n",
        "        # p_current_class stores the probability of the current class\n",
        "        p_current_class = probability(dictionary, x, current_class)\n",
        "        # here we check whether the probability of the current class is greater\n",
        "        # than the best probability that we have got till now\n",
        "        if (first_run or p_current_class > best_p):\n",
        "            best_p = p_current_class\n",
        "            best_class = current_class\n",
        "        first_run = False\n",
        "    return best_class"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3D21mN9FHDJy",
        "colab": {}
      },
      "source": [
        "def predict(dictionary, X_test):\n",
        "    y_pred = []\n",
        "    count=0\n",
        "    for x in X_test:\n",
        "        # for each point we predict the class and append this class to y_predict\n",
        "        x_class = predictSinglePoint(dictionary, x)\n",
        "        print(x_class,count)\n",
        "        count+=1\n",
        "        y_pred.append(x_class)\n",
        "    return y_pred"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tp7gTJT-ON9X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "fe744f4a-49b2-4546-9c1b-7a0205d07a45"
      },
      "source": [
        "print(a.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11314, 2000)\n",
            "(11314, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bpTKluRLX941",
        "colab": {}
      },
      "source": [
        "# converting the sparse matrix a to dataframe\n",
        "a = pd.DataFrame(a.toarray())"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x0row3zUQ0y0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8f9bb5ee-0eee-4bbb-8a9a-86a06904a124"
      },
      "source": [
        "# calling the fit function on a and y_train\n",
        "dictionary = fit(a,y_train)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'rec.sport.baseball', 'talk.politics.mideast', 'rec.autos', 'talk.religion.misc', 'sci.crypt', 'comp.graphics', 'talk.politics.misc', 'soc.religion.christian', 'comp.sys.ibm.pc.hardware', 'comp.windows.x', 'sci.space', 'misc.forsale', 'comp.sys.mac.hardware', 'talk.politics.guns', 'comp.os.ms-windows.misc', 'rec.sport.hockey', 'sci.electronics', 'sci.med', 'rec.motorcycles', 'alt.atheism'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bCaAAEKEizyY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "00028f11-dc6c-4467-8e6e-3f67480631ef"
      },
      "source": [
        "z='rec.sport.baseball'\n",
        "print(dictionary[z][\"total_count\"])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "54907\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tJeJ7_8kIhe5",
        "colab": {}
      },
      "source": [
        "# calling the  predict function on dictionary and test_documents\n",
        "Y_pred = predict(dictionary,test_documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CcWtfdpWqq9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "67ac4c44-7e95-4f1a-d71a-9584da4b9d69"
      },
      "source": [
        "print(len(Y_pred))\n",
        "print(len(y_test))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7532\n",
            "7532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9wJEwvEjqvLX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "19eed266-9c30-418e-8ed4-33e0325501ba"
      },
      "source": [
        "# this is classification_report and confusion matrix for my implementation of\n",
        "# Naive Bayes\n",
        "print(classification_report(y_test,Y_pred))\n",
        "print(confusion_matrix(y_test,Y_pred))\n",
        "# From the confusion matrix i can see that I got an accuracy of 0.73"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.65      0.55      0.59       319\n",
            "           comp.graphics       0.49      0.71      0.58       389\n",
            " comp.os.ms-windows.misc       0.50      0.00      0.01       394\n",
            "comp.sys.ibm.pc.hardware       0.46      0.60      0.52       392\n",
            "   comp.sys.mac.hardware       0.60      0.68      0.63       385\n",
            "          comp.windows.x       0.67      0.66      0.66       395\n",
            "            misc.forsale       0.76      0.75      0.76       390\n",
            "               rec.autos       0.74      0.81      0.77       396\n",
            "         rec.motorcycles       0.82      0.85      0.83       398\n",
            "      rec.sport.baseball       0.77      0.80      0.79       397\n",
            "        rec.sport.hockey       0.94      0.80      0.86       399\n",
            "               sci.crypt       0.86      0.82      0.84       396\n",
            "         sci.electronics       0.59      0.64      0.61       393\n",
            "                 sci.med       0.76      0.68      0.72       396\n",
            "               sci.space       0.84      0.79      0.81       394\n",
            "  soc.religion.christian       0.64      0.83      0.72       398\n",
            "      talk.politics.guns       0.65      0.81      0.72       364\n",
            "   talk.politics.mideast       0.90      0.78      0.84       376\n",
            "      talk.politics.misc       0.62      0.53      0.57       310\n",
            "      talk.religion.misc       0.42      0.47      0.44       251\n",
            "\n",
            "                accuracy                           0.69      7532\n",
            "               macro avg       0.68      0.68      0.66      7532\n",
            "            weighted avg       0.69      0.69      0.67      7532\n",
            "\n",
            "[[175   2   0   0   0   0   0   3   6   4   0   1   1   6   1  54   7  12\n",
            "    6  41]\n",
            " [  2 278   0  16  14  24   6   3   2   2   1   2  12  12   7   4   0   1\n",
            "    0   3]\n",
            " [  1 104   1 131  28  83   9   1   1   3   0   3   4   5   3   0   0   0\n",
            "    5  12]\n",
            " [  1  23   0 237  72   5  12   4   1   1   0   2  32   2   0   0   0   0\n",
            "    0   0]\n",
            " [  1  13   0  51 261   3  10   7   2   2   0   1  17   6   8   1   0   1\n",
            "    0   1]\n",
            " [  0  71   1  11   6 259  10   2   1   2   1   5   6   7   3   3   1   0\n",
            "    0   6]\n",
            " [  0   5   0  30  18   0 294  17   6   1   1   1   9   3   0   1   0   0\n",
            "    1   3]\n",
            " [  0   2   0   1   4   1   9 322  17   3   0   1  20   4   3   1   1   0\n",
            "    2   5]\n",
            " [  0   2   0   2   1   0   2  21 337   3   0   0   8   6   0   0   7   1\n",
            "    7   1]\n",
            " [  3   4   0   0   0   0  13   8   5 318  15   1   9   0   2   4   3   1\n",
            "    8   3]\n",
            " [  2   0   0   0   1   0   1   3   3  58 320   0   1   1   1   3   1   0\n",
            "    2   2]\n",
            " [  1  10   0   1   3   1   1   2   3   1   0 326  14   2   2   0   9   2\n",
            "   13   5]\n",
            " [  0  20   0  31  24   3   7  15  11   0   1  20 250   4   3   1   0   1\n",
            "    1   1]\n",
            " [  9  13   0   0   4   1   6  18   5   2   0   0  21 271   7  14   3   1\n",
            "   11  10]\n",
            " [  3  13   0   0   1   1   1   4   4   2   1   1  10  15 310   8   4   0\n",
            "   13   3]\n",
            " [ 17   4   0   0   0   2   0   2   0   2   0   1   5   1   3 330   1   2\n",
            "    1  27]\n",
            " [  1   0   0   0   1   1   3   4   2   2   0   9   3   2   0   6 295   3\n",
            "   10  22]\n",
            " [ 21   1   0   0   0   1   2   0   4   2   0   1   2   0   3  18   5 295\n",
            "   15   6]\n",
            " [  4   2   0   0   0   0   1   0   2   3   1   3   0   6   9   7  98   2\n",
            "  164   8]\n",
            " [ 29   4   0   0   0   0   0   1   0   0   1   1   2   2   3  63  20   4\n",
            "    4 117]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-bhBQbTirXXZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "320b54d3-89fe-4dcd-ed67-b6ecdcfc354b"
      },
      "source": [
        "# Here i have used Multinomial Naive Bayes already implemented in SKlearn\n",
        "clf = MultinomialNB()\n",
        "clf.fit(a, y_train)\n",
        "# filename = 'finalized_model.sav'\n",
        "# joblib.dump(model, filename)\n",
        "Y_pred = clf.predict(x_test_features)\n",
        "print(classification_report(y_test,Y_pred))\n",
        "print(confusion_matrix(y_test,Y_pred))\n",
        "# I can see from the classifiation report and confusion matrix that here i have \n",
        "# got an accurcy of 0.88"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.65      0.66      0.65       319\n",
            "           comp.graphics       0.48      0.72      0.58       389\n",
            " comp.os.ms-windows.misc       0.50      0.00      0.01       394\n",
            "comp.sys.ibm.pc.hardware       0.45      0.59      0.51       392\n",
            "   comp.sys.mac.hardware       0.57      0.69      0.62       385\n",
            "          comp.windows.x       0.72      0.66      0.69       395\n",
            "            misc.forsale       0.65      0.83      0.73       390\n",
            "               rec.autos       0.67      0.79      0.72       396\n",
            "         rec.motorcycles       0.66      0.89      0.76       398\n",
            "      rec.sport.baseball       0.74      0.80      0.77       397\n",
            "        rec.sport.hockey       0.93      0.75      0.83       399\n",
            "               sci.crypt       0.91      0.78      0.84       396\n",
            "         sci.electronics       0.57      0.61      0.59       393\n",
            "                 sci.med       0.81      0.63      0.71       396\n",
            "               sci.space       0.82      0.80      0.81       394\n",
            "  soc.religion.christian       0.86      0.83      0.85       398\n",
            "      talk.politics.guns       0.66      0.79      0.72       364\n",
            "   talk.politics.mideast       0.95      0.73      0.82       376\n",
            "      talk.politics.misc       0.70      0.46      0.56       310\n",
            "      talk.religion.misc       0.45      0.47      0.46       251\n",
            "\n",
            "                accuracy                           0.68      7532\n",
            "               macro avg       0.69      0.67      0.66      7532\n",
            "            weighted avg       0.69      0.68      0.67      7532\n",
            "\n",
            "[[211   2   0   2   1   0   2  10   9   2   0   1   2   3   1  14   9   4\n",
            "    3  43]\n",
            " [  2 281   0  10  22  19   8   6   3   3   0   3  14   6   8   0   1   1\n",
            "    0   2]\n",
            " [  1  91   1 151  38  60  17   3   8   3   0   1   5   2   5   0   0   0\n",
            "    0   8]\n",
            " [  0  23   0 231  62   4  23   5   1   1   0   2  39   0   1   0   0   0\n",
            "    0   0]\n",
            " [  0  10   0  47 265   0  23   6   3   3   0   1  17   3   7   0   0   0\n",
            "    0   0]\n",
            " [  0  81   1  18   8 261   8   2   3   3   0   2   1   3   3   0   1   0\n",
            "    0   0]\n",
            " [  0   6   0  21  12   1 322  10   6   0   2   0   7   0   2   0   0   0\n",
            "    0   1]\n",
            " [  0   2   0   0   5   1  18 311  26   3   0   0  19   1   6   0   1   1\n",
            "    1   1]\n",
            " [  0   0   0   1   3   0   7  15 353   2   1   0   4   2   1   0   5   0\n",
            "    3   1]\n",
            " [  2   3   0   0   3   0  15  11  11 316  17   0   8   1   2   0   4   0\n",
            "    3   1]\n",
            " [  0   2   0   0   5   0   3   2  13  67 298   0   1   1   2   0   0   0\n",
            "    3   2]\n",
            " [  1  10   0   2   6   1   7   8  12   1   0 308  19   0   3   0   6   0\n",
            "   11   1]\n",
            " [  0  30   0  33  26   2  11  12  21   2   1   7 241   4   3   0   0   0\n",
            "    0   0]\n",
            " [ 15   9   0   0   7   2  12  25  23   4   2   0  21 248   4   2   4   2\n",
            "    7   9]\n",
            " [  2  18   0   0   2   0   4   7   5   4   0   0   9  12 316   3   5   0\n",
            "    6   1]\n",
            " [ 12   6   0   0   0   1   1   1   4   2   0   0   6   0   3 331   2   1\n",
            "    1  27]\n",
            " [  1   1   0   1   1   0   4  15  15   3   0   7   3   4   2   1 288   1\n",
            "    5  12]\n",
            " [ 32   1   0   0   3   6   3   2   5   6   0   2   1   4   2   2  10 273\n",
            "   12  12]\n",
            " [  1   3   0   0   0   0   3  12  13   2   0   3   2   5  12   5  82   1\n",
            "  144  22]\n",
            " [ 47   3   0   0   0   2   2   4   3   2   1   0   3   7   2  27  20   3\n",
            "    8 117]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}